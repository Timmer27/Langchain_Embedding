{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 허깅페이스 Lanchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill-mask 등 현재 langchain과 huggingface 연동이 지원되지 않을 때 직접 pipeline 구축하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\whdgh\\.cache\\huggingface\\hub\\models--bigscience--bloomz-7b1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloomz-7b1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:626: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  for Premier League club Tottenham Hotspur and captains\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Model and tokenizer\n",
    "model_name = 'bigscience/bloomz-7b1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Question answering pipeline\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Question and context\n",
    "question = \"Who is Son Heung Min?\"\n",
    "context = \"\"\"\n",
    "    Son Heung-min is a South Korean professional footballer who plays as a forward for Premier League club Tottenham Hotspur and captains the South Korea national team. \n",
    "    Considered one of the best forwards in the world, he is known for his explosive speed, finishing, and two-footed ability.\n",
    "\"\"\"\n",
    "\n",
    "# Answering the question\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### huggingface와 langchain pipeline 지원될 때 아래와 같이 연동하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Son Heung Min?\n",
      "\n",
      "Answer:  Son Heung-min is a South Korean professional footballer who plays as a winger for English Premier League club Tottenham Hotspur and the South Korea national team. He is known for his pace, dribbling skills, and ability to score goals. Son joined Tottenham from Bayer Leverkusen in 2015 and has since become a key player for the club, helping them to reach the Champions League final in 2019. He\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = config['HUGGINGFACE_API']['HUGGINGFACEHUB_API_TOKEN']\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# HuggingFace Repository ID\n",
    "repo_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "# repo_id = 'bigscience/bloomz-7b1'\n",
    "\n",
    "# 질의내용\n",
    "question = \"Who is Son Heung Min?\"\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# HuggingFaceHub 객체 생성\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    model_kwargs={\"temperature\": 0.2, \n",
    "                  \"max_length\": 128}\n",
    ")\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 실행\n",
    "print(llm_chain.run(question=question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
