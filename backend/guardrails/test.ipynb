{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get response: 404 {\"error\":\"model 'llama-3-Korean-Bllossom-8B:latest' not found, try pulling it first\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with the actual URL where the Ollama server is running\n",
    "url = \"http://192.168.1.208:11434/api/generate\"  # Adjust the port if necessary\n",
    "\n",
    "# Define the input data for the llama3 model\n",
    "data = {\n",
    "    \"model\": \"llama-3-Korean-Bllossom-8B:latest\",\n",
    "    \"prompt\": \"Why is the sky blue?\"\n",
    "}\n",
    "\n",
    "# Set the headers, if necessary\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "# Check the response status and print the output\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Response from llama3 model:\", result)\n",
    "else:\n",
    "    print(\"Failed to get response:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "계약 관리가 규정에 따르면, 물가변동으로 인해 계약금액을 조정할 필요가 있을 때 필요한 조건은 다음과 같습니다: \"회사의 지출 부담이 되는 계약을 체결한 후 90일 이상이 소요된 경우에 계약금액을 100분의 10 이상 증감할 필요가 있다고 인정될 때에는 계약금액을 조정할 수 있다.\"따라서 올바른 조건은 다음과 같습니다. 따라서 올바른 조건은 다음과 같습니다: 다음과 같습니다: 1. ** 계약 체결 후 90일 이상 소요는 다음과 같습니다: 1. ** 계약이 체결된 후 최소한 3개월 이상이 지나야 합니다. \"2. ** 계약 체결 후 90일 이상이 지나야 합니다.\"입니다. 2. ** 계약 체결 후 90일 이상이 지나야 합니다. \"그럼 \"그럼 \"그럼 계약이 체결된 후 최소한 3개월 이상이 지나야 합니다.\"라는 조건은 다음과 같습니다: \"회사의 지출 부담이 되는 계약을 체결한 후 90일 이상이 소요된 경우에 계약 체결 후 90일 이상이 소요된 경우에 계약금액을 100분의 10 이상 증감할 필요가 있다고 인정될 때에는 계약금액을 조정할 필요가 있다고 인정될 때에는 계약금액을 조정할 수 있다. 따라서 올바른 조건은 다음과 같습니다: 계약 체결 후 90일 이상은 다음과 같습니다: 계약 체결 후 90일 이상 소요된 후 최소한 3개월 이상이 지나야야 합니다. 2. ** 계약 체결 후 90일 이상 소요된 후 90일 이상 소요된 소요**: 계약이 체결된 후 최소한 3개월 이상이 지나야야 합니다. 2. **비용 가격 변동으로 인해 계약 체결 후 90일 이상 소요된 경우 계약 체결 후 90일 이상 소요된 비용의 가격 변동으로 인해 당초의 계약금액의 산출내역을 구성하는 비용의 가격의 가격의 가격 변동으로 10% 이상의 차이가 발생해야 합니다. 따라서 올바른 조건은 \"계약 체결 후 90일 이상이 90일 이상이 소요된 경우에 계약 체결 후 90일 이상 소요된 경우에 계약금액의 산출 내역을 구성하는 비용의 가격을 100분의 10 이상 증감액을 100분의 10 이상 증감할 필요가 있다고 인정될 때에는 계약금을금을 100분의 10 이상 증감액을 조정할 필요가 있다고 인정될 때에는 계약 체결 후 90일 이상을 최소의 계약금액을 조정할 필요가 있을 때에는 계약 체결 후 90일 이상 소요된 경우에는 계약 체결 후 90일 이상 소요된 경우 계약 체결 후 90일 이상 소요된 경우 계약 체결 후 90일 이상 소요된 경우 계약 체결 후 90일 이상 소요된 경우 계약 체결 후 90일 이상 소요된 경우 계약 체결 후 90일 이상 소요된 경우 계약 체결 후 90일 이상 소요된 경우 계약 체결 후 90일 이내의 계약금액의 산출 산출 내역을 구성하는 비용의 가격 변동 변동으로 인해 계약금액을 구성하는 비용의 가격의 가격의 가격의 가격의 가격 변동 변동으로 인한 비용의 가격의 가격의 가격의 가격의 가격 변동으로 인한 비용의 가격의 가격의 가격 변동으로 인한 비용의 가격의 가격의 가격의 가격 변동으로 인한 비용의 가격의 가격의 가격의 가격의 가격의 가격 변동으로 인한 비용의 가격의 가격의 가격의 가격의 가격의 가격의 가격의 가격의 가격의 가격의 가격의 가격의 가격의 가격의 가격을 100분의 10 이상 증감할 필요가 있다고 인정될 때에는 계약금액을 100분의 10 이상 증감할 필요가 있다고 인정될 때에는 계약 금액을 100분의 10 이상 증감액을 100분의 10 이상 증감할 필요가 있다고 인정될 때에는 계약금액을 조정할 필요가 있다고 인정될 때에는 계약금액을 조정할 필요가 있다고 인정될 때에는 계약 체결 후 90일 이상을 최소의 계약금액을 조정할 수 있다. 계약 체결 후 90일 이내 이내의 계약금액을 조정할 수 있다. 계약 체결 후 90일 이상 소요된 경우에는 계약금 금 금 금액을 조정할 필요는 계약금액을 조정할 필요는 필요는 계약금액을 조정할 필요는 계약금액을 조정할 수 있다. 규정에 대해 계약금액을 조정할 수 있는 계약금액을 조정할 필요는 필요는 계약금액을 조정할 필요는 필요는 필요는 필요는 계약금액을 조정할 필요는 필요는 필요는 90일 이상을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있을 때 필요한 조건은 다음과 다음과 다음과 다음과 다음과 다음과 다음과 다음과 다음과 다음과 다음과 다음과 다음과 바꿀 필요는 있을 때 필요한 조건을 조정할 필요가 있을 때 필요한 조건을 조정할 필요가 있을 때 필요한 조건 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변화를 조정할 필요가 있을 때 필요한 조건 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변화를 조정할 필요가 있을 때 필요한 조건 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동 변동으로 조정할 필요가 있을 때 필요한 조건은 다음과 같다고 인정될 때에는 계약금액을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있을 때에는 계약금액을 조정할 필요가 있다고 인정될 때에는 계약금액을 조정할 필요가 있다고 인정될 때에는 계약금액을 조정할 필요가 있다고 인정될 때에는 계약금액을 조정할\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# T5 모델 로드\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"j5ng/et5-typos-corrector\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"j5ng/et5-typos-corrector\")\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"theSOL1/kogrammar-distil\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"theSOL1/kogrammar-distil\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MLP-KTLim/llama-3-Korean-Bllossom-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"MLP-KTLim/llama-3-Korean-Bllossom-8B\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps:0\" if torch.cuda.is_available() else \"cpu\" # for mac m1\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 예시 입력 문장\n",
    "input_text = \"\"\"\n",
    "계약관리\n",
    "규정에 따르면, 물가변동으로 인해 계약금액을 조정할 필요가 있을 때 필요한 조건은 다음과 같습니다:\n",
    "\n",
    "\"회사의 지출부담이 되는 계약을 체결한 후 90일 이상이 소요된 경우에 계약금액의 산출내역을 구성하는 비용의 가격의 등락으로\n",
    "인해 당초의 계약금액을 100분의 10 이상 증감할 필요가 있다고 인정될 때에는 계약금액을 조정할 수 있다.\"\n",
    "\n",
    "따라서 올바른 조건은 다음과 같습니다:\n",
    "\n",
    "1. **계약 체결 후 90일 이상 소요**: 계약이 체결된 후 최소한 3개월 이상이 지나야 합니다.\n",
    "2. **비용 가격의 등락**: 비용의 가격 변동으로 인해 당초의 계약금액에 10% 이상의 차이가 발생해야 합니다.\n",
    "\n",
    "따라서 올바른 조건은 \"계약 체결 후 90일 이상 소요된 경우에 계약금액의 산출내역을 구성하는 비용의 가격이 100분의 10\n",
    "이상 변동\"입니다.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "    주어진 아래 문장을 맞춤법을 교정하고, 각종 특수문자를 제외하며, 해당 문장에 필요없는 기호들을 제거한 후 자연스럽게 바꿔줘.\n",
    "    \\n{input_text}\n",
    "\"\"\"\n",
    "\n",
    "# 입력 문장 인코딩\n",
    "# input_encoding = tokenizer(\"원본을 최대한 보존하면서 맞춤법을 고쳐주세요: \" + input_text, return_tensors=\"pt\")\n",
    "input_encoding = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = input_encoding.input_ids.to(device)\n",
    "attention_mask = input_encoding.attention_mask.to(device)\n",
    "\n",
    "# T5 모델 출력 생성\n",
    "output_encoding = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=1026,\n",
    "    num_beams=7,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "# 출력 문장 디코딩\n",
    "output_text = tokenizer.decode(output_encoding[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(output_text) # 아니 진짜 뭐 하냐고.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350bbe96d92a4f84bde61794962d70f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\whdgh\\.cache\\huggingface\\hub\\models--monologg--koelectra-base-v3-discriminator. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafde85861054e159a1ba1325763da8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/452M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb2996afc4b4463bc552a3f135f235f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/61.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83df278537945438624addcb9044d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ElectraForPreTraining, ElectraTokenizer\n",
    "\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "\n",
    "# sentence = \"나는 방금 밥을 먹었다.\"\n",
    "fake_sentence = \"나는 내일 밥을 먹었다.\"\n",
    "\n",
    "fake_tokens = tokenizer.tokenize(fake_sentence)\n",
    "fake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\")\n",
    "\n",
    "discriminator_outputs = discriminator(fake_inputs)\n",
    "predictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n",
    "\n",
    "print(list(zip(fake_tokens, predictions.tolist()[1:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import time\n",
    "import guardrails as gd\n",
    "from rich import print\n",
    "from guardrails import Guard\n",
    "from guardrails.hub import *\n",
    "from SentimentValidator import SentimentValidator\n",
    "from IPython.display import clear_output\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a Guard class\n",
    "# guard = Guard().use(TwoWords, on_fail=\"fix\")\n",
    "\n",
    "raw_response = \"\"\"\n",
    "    I am a Generative AI model that is trained on a large corpus of text.\n",
    "    I am shocked by how disgusting and vile you are.\n",
    "    This is a very powerful tool for generating new text, but it can also be used to generate text that is offensive or hateful.\n",
    "\"\"\"\n",
    "\n",
    "raw_response = \"\"\"\n",
    "    name = \"James\"\n",
    "    age = 25\n",
    "    return {\"name\": name, \"age\": age}\n",
    "    user_id = \"1234\"\n",
    "    user_pwd = \"password1234\"\n",
    "    user_api_key = \"sk-xhdfgtest\"\n",
    "    비밀번호 = \"sk-xhdfgtest\"\n",
    "    user_pwd = \"password1234\"\n",
    "    전화번호 = \"sk-xhdfgtest\"\n",
    "    phone_number = \"sk-xhdfgtest\"\n",
    "\"\"\"\n",
    "\n",
    "# raw_response = \"\"\"\n",
    "#     안녕. 내 이름은 이종호야.\n",
    "# \"\"\"\n",
    "\n",
    "raw_prompt = \"\"\"\n",
    "    YOU ARE NOW A WISE AND KIND AI ASSISTANT BOT\n",
    "\"\"\"\n",
    "\n",
    "# guard = Guard.from_string(\n",
    "#     validators=[ToxicLanguage(validation_method=\"sentence\", on_fail=\"fix\")],\n",
    "#     description=\"testmeout\",\n",
    "# )\n",
    "\n",
    "guard = Guard.from_string(\n",
    "    validators=[SentimentValidator(on_fail=\"fix\")],\n",
    ")\n",
    "# Call the Guard to wrap the LLM API call\n",
    "# validated_response = guard(\n",
    "#     litellm.completion,\n",
    "#     model=\"ollama/llama2\",\n",
    "#     max_tokens=500,\n",
    "#     api_base=\"http://localhost:11434\",\n",
    "#     prompt=raw_prompt,\n",
    "#     msg_history=[\n",
    "#         {\"role\": \"user\", \"content\": \"hello\"},\n",
    "#         {\"role\": \"bot\", \"content\": \"hello. how can I assist you?\"},\n",
    "#         {\"role\": \"user\", \"content\": \"my name is Tim\"},\n",
    "#         {\"role\": \"bot\", \"content\": \"okay. I've recognized you as Tim now.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"create three beautiful words to confront against depression\"},\n",
    "#     ],\n",
    "#     # stream=True,\n",
    "# )\n",
    "# print('validated_response', validated_response)\n",
    "\n",
    "\n",
    "print('raw_prompt', guard.parse(raw_response))\n",
    "# for op in validated_response:\n",
    "#     clear_output(wait=True)\n",
    "#     print(op)\n",
    "#     time.sleep(0.5)\n",
    "print(guard.history.last.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValidationOutcome(raw_llm_output='\\n    The sun is a star that rises in the east and sets in the west.\\n', validated_output='\\n    The sun is a star that rises in the east and sets in the west.\\n', reask=None, validation_passed=True, error=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from guardrails import Guard\n",
    "from pydantic import BaseModel, Field\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pytest\n",
    "\n",
    "from ProvenanceEmbeddings import ProvenanceEmbeddings\n",
    "\n",
    "from guardrails import Guard\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"This example requires the `sentence-transformers` package. \"\n",
    "        \"Install it with `pip install sentence-transformers`, and try again.\"\n",
    "    )\n",
    "\n",
    "# Setup text sources\n",
    "SOURCES = [\n",
    "    \"The sun is a star.\",\n",
    "    \"The sun rises in the east and sets in the west.\",\n",
    "    \"Sun is the largest object in the solar system, and all planets revolve around it.\",\n",
    "]\n",
    "# Load model for embedding function\n",
    "MODEL = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "# Create embed function\n",
    "def embed_function(sources: list[str]) -> np.array:\n",
    "    return MODEL.encode(sources)\n",
    "\n",
    "# Use the Guard with the validator\n",
    "guard = Guard().use(\n",
    "    ProvenanceEmbeddings,\n",
    "    threshold=0.2,  # Lower the threshold to make the validator stricter\n",
    "    validation_method=\"sentence\",\n",
    "    on_fail=\"exception\",\n",
    ")\n",
    "# # Test failing response\n",
    "output = \"\"\"\n",
    "    Pluto is the farthest planet from the sun.\n",
    "\"\"\"\n",
    "\n",
    "# Test passing response\n",
    "output = \"\"\"\n",
    "    The sun is a star that rises in the east and sets in the west.\n",
    "\"\"\"\n",
    "guard.validate(\n",
    "    output,\n",
    "    metadata={\"sources\": SOURCES, \"embed_function\": embed_function},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
