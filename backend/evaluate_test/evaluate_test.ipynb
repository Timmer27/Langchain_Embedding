{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_name</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>metrics</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt</td>\n",
       "      <td>query</td>\n",
       "      <td>answer</td>\n",
       "      <td>context\\n\\ncontext</td>\n",
       "      <td>1</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  llm_name  query  answer             context  metrics  reason\n",
       "0      gpt  query  answer  context\\n\\ncontext        1  reason"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = [\n",
    "    {'llm_name': 'gpt', 'query': 'query', 'answer': 'answer', 'context': ('\\n\\n'.join(['context', 'context'])), 'metrics': 1, 'reason': 'reason'},\n",
    "    ]\n",
    "pd.DataFrame(test).to_csv('./')\n",
    "# '\\n\\n'.join(['context', 'context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00ac88cd3b449dd9527575eb78da25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import statistics\n",
    "import os\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "os.environ[\"OPENAI_API_KEY\"] = config['API']['OPENAI_API_KEY']\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "res = metric.measure(test_case)\n",
    "# print(metric.score)\n",
    "# print(metric.reason)\n",
    "res\n",
    "# or evaluate test cases in bulk\n",
    "# evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label value\n",
      "label value\n",
      "label value\n",
      "label value\n",
      "label value\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    {'label': 'FAITHFUL', 'value': 'FAITHFUL'},\n",
    "    {'label': 'CONTEXTUAL_PRECISION', 'value': 'CONTEXTUAL_PRECISION'},\n",
    "    {'label': 'CONTEXTUAL_RELAVANCY', 'value': 'CONTEXTUAL_RELAVANCY'},\n",
    "    {'label': 'CONTEXTUAL_RECALL', 'value': 'CONTEXTUAL_RECALL'},\n",
    "    {'label': 'ANSWER_RELAVANCY', 'value': 'ANSWER_RELAVANCY'}\n",
    "]\n",
    "\n",
    "for label, value in metrics:\n",
    "    print(label, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\deepeval\\__init__.py:42: UserWarning: You are using deepeval version 0.21.47, however version 0.21.57 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat ran up the tree."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The cat ran up the tree.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "import statistics\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = config['API']['OPENAI_API_KEY']\n",
    "\n",
    "from deepeval.metrics import *\n",
    "from deepeval.test_case import *\n",
    "\n",
    "from deepeval.benchmarks import *\n",
    "from deepeval.benchmarks.tasks import *\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "input = \"The dog chased the cat up the tree, who ran up the tree?\"\n",
    "expected_output = \"cat.\"\n",
    "local_path = './models/nous-hermes-llama2-13b.Q4_0.gguf'\n",
    "\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    verbose=True,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.7,\n",
    ")\n",
    "llm_chain = (                     \n",
    "    PromptTemplate(input_variables=[\"question\"], template=input)\n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "actual_output = llm_chain.invoke({\"question\": \"\"})\n",
    "actual_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'input': 'Let x = 1. What is x << 3 in Python 3?\\nA. 1\\nB. 3\\nC. 8\\nD. 16\\nAnswer:',\n",
       "   'expected_output': 'C'},\n",
       "  {'input': 'In Python 3, which of the following function convert a string to an int in python?\\nA. int(x [,base])\\nB. long(x [,base] )\\nC. float(x)\\nD. str(x)\\nAnswer:',\n",
       "   'expected_output': 'A'},\n",
       "  {'input': \"A user enters a Web address in a browser, and a request for a file is sent to a Web server. Which of the following best describes how the file is sent to the user?\\nA. The file is broken into packets for transmission. The packets must be reassembled upon receipt.\\nB. The file is broken into packets for transmission. The user's browser must request each packet in order until all packets are received.\\nC. The server attempts to connect directly to the user's computer. If the connection is successful, the entire file is sent. If the connection is unsuccessful, an error message is sent to the user.\\nD. The server repeatedly attempts to connect directly to the user's computer until a connection is made. Once the connection is made, the entire file is sent.\\nAnswer:\",\n",
       "   'expected_output': 'A'}],\n",
       " 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu_benchmark = MMLU().load_benchmark_dataset(MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE)\n",
    "data = [{'input': mmlu.input, 'expected_output': mmlu.expected_output} for mmlu in mmlu_benchmark]\n",
    "data[:3], len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\n",
    "    'APPLYING_SUNSCREEN',\n",
    "    'TRIMMING_BRANCHES_OR_HEDGES',\n",
    "    'DISC_DOG',\n",
    "    'WAKEBOARDING',\n",
    "    'SKATEBOARDING',\n",
    "    'WATERSKIING',\n",
    "    'WASHING_HANDS',\n",
    "    'SAILING',\n",
    "    'PLAYING_CONGAS',\n",
    "    'BALLET',\n",
    "    'ROOF_SHINGLE_REMOVAL',\n",
    "    'HAND_CAR_WASH',\n",
    "    'KITE_FLYING',\n",
    "    'PLAYING_POOL',\n",
    "    'PLAYING_LACROSSE',\n",
    "    'LAYUP_DRILL_IN_BASKETBALL',\n",
    "    'HOME_AND_GARDEN',\n",
    "    'PLAYING_BEACH_VOLLEYBALL',\n",
    "    'CALF_ROPING',\n",
    "    'SCUBA_DIVING',\n",
    "    'MIXING_DRINKS',\n",
    "    'PUTTING_ON_SHOES',\n",
    "    'MAKING_A_LEMONADE',\n",
    "    'UNCATEGORIZED',\n",
    "    'ZUMBA',\n",
    "    'PLAYING_BADMINTON',\n",
    "    'PLAYING_BAGPIPES',\n",
    "    'FOOD_AND_ENTERTAINING',\n",
    "    'PERSONAL_CARE_AND_STYLE',\n",
    "    'CRICKET',\n",
    "    'SHOVELING_SNOW',\n",
    "    'PING_PONG',\n",
    "    'HOLIDAYS_AND_TRADITIONS',\n",
    "    'ICE_FISHING',\n",
    "    'BEACH_SOCCER',\n",
    "    'TABLE_SOCCER',\n",
    "    'SWIMMING',\n",
    "    'BATON_TWIRLING',\n",
    "    'JAVELIN_THROW',\n",
    "    'SHOT_PUT',\n",
    "    'DOING_CRUNCHES',\n",
    "    'POLISHING_SHOES',\n",
    "    'TRAVEL',\n",
    "    'USING_UNEVEN_BARS',\n",
    "    'PLAYING_HARMONICA',\n",
    "    'RELATIONSHIPS',\n",
    "    'HIGH_JUMP',\n",
    "    'MAKING_A_SANDWICH',\n",
    "    'POWERBOCKING',\n",
    "    'REMOVING_ICE_FROM_CAR',\n",
    "    'SHAVING',\n",
    "    'SHARPENING_KNIVES',\n",
    "    'WELDING',\n",
    "    'USING_PARALLEL_BARS',\n",
    "    'HOME_CATEGORIES',\n",
    "    'ROCK_CLIMBING',\n",
    "    'SNOW_TUBING',\n",
    "    'WASHING_FACE',\n",
    "    'ASSEMBLING_BICYCLE',\n",
    "    'TENNIS_SERVE_WITH_BALL_BOUNCING',\n",
    "    'SHUFFLEBOARD',\n",
    "    'DODGEBALL',\n",
    "    'CAPOEIRA',\n",
    "    'PAINTBALL',\n",
    "    'DOING_A_POWERBOMB',\n",
    "    'DOING_MOTOCROSS',\n",
    "    'PLAYING_ICE_HOCKEY',\n",
    "    'PHILOSOPHY_AND_RELIGION',\n",
    "    'ARCHERY',\n",
    "    'CARS_AND_OTHER_VEHICLES',\n",
    "    'RUNNING_A_MARATHON',\n",
    "    'THROWING_DARTS',\n",
    "    'PAINTING_FURNITURE',\n",
    "    'HAVING_AN_ICE_CREAM',\n",
    "    'SLACKLINING',\n",
    "    'CAMEL_RIDE',\n",
    "    'ARM_WRESTLING',\n",
    "    'HULA_HOOP',\n",
    "    'SURFING',\n",
    "    'PLAYING_PIANO',\n",
    "    'GARGLING_MOUTHWASH',\n",
    "    'PLAYING_ACCORDION',\n",
    "    'HORSEBACK_RIDING',\n",
    "    'PUTTING_IN_CONTACT_LENSES',\n",
    "    'PLAYING_SAXOPHONE',\n",
    "    'FUTSAL',\n",
    "    'LONG_JUMP',\n",
    "    'LONGBOARDING',\n",
    "    'POLE_VAULT',\n",
    "    'BUILDING_SANDCASTLES',\n",
    "    'PLATFORM_DIVING',\n",
    "    'PAINTING',\n",
    "    'SPINNING',\n",
    "    'CARVING_JACK_O_LANTERNS',\n",
    "    'BRAIDING_HAIR',\n",
    "    'YOUTH',\n",
    "    'PLAYING_VIOLIN',\n",
    "    'CANOEING',\n",
    "    'CHEERLEADING',\n",
    "    'PETS_AND_ANIMALS',\n",
    "    'KAYAKING',\n",
    "    'CLEANING_SHOES',\n",
    "    'KNITTING',\n",
    "    'BAKING_COOKIES',\n",
    "    'DOING_FENCING',\n",
    "    'PLAYING_GUITARRA',\n",
    "    'USING_THE_ROWING_MACHINE',\n",
    "    'GETTING_A_HAIRCUT',\n",
    "    'MOOPING_FLOOR',\n",
    "    'RIVER_TUBING',\n",
    "    'CLEANING_SINK',\n",
    "    'GROOMING_DOG',\n",
    "    'DISCUS_THROW',\n",
    "    'CLEANING_WINDOWS',\n",
    "    'FINANCE_AND_BUSINESS',\n",
    "    'HANGING_WALLPAPER',\n",
    "    'ROPE_SKIPPING',\n",
    "    'WINDSURFING',\n",
    "    'KNEELING',\n",
    "    'GETTING_A_PIERCING',\n",
    "    'ROCK_PAPER_SCISSORS',\n",
    "    'SPORTS_AND_FITNESS',\n",
    "    'BREAKDANCING',\n",
    "    'WALKING_THE_DOG',\n",
    "    'PLAYING_DRUMS',\n",
    "    'PLAYING_WATER_POLO',\n",
    "    'BMX',\n",
    "    'SMOKING_A_CIGARETTE',\n",
    "    'BLOWING_LEAVES',\n",
    "    'BULLFIGHTING',\n",
    "    'DRINKING_COFFEE',\n",
    "    'BATHING_DOG',\n",
    "    'TANGO',\n",
    "    'WRAPPING_PRESENTS',\n",
    "    'PLASTERING',\n",
    "    'PLAYING_BLACKJACK',\n",
    "    'FUN_SLIDING_DOWN',\n",
    "    'WORK_WORLD',\n",
    "    'TRIPLE_JUMP',\n",
    "    'TUMBLING',\n",
    "    'SKIING',\n",
    "    'DOING_KICKBOXING',\n",
    "    'BLOW_DRYING_HAIR',\n",
    "    'DRUM_CORPS',\n",
    "    'SMOKING_HOOKAH',\n",
    "    'MOWING_THE_LAWN',\n",
    "    'VOLLEYBALL',\n",
    "    'LAYING_TILE',\n",
    "    'STARTING_A_CAMPFIRE',\n",
    "    'SUMO',\n",
    "    'HURLING',\n",
    "    'PLAYING_KICKBALL',\n",
    "    'MAKING_A_CAKE',\n",
    "    'FIXING_THE_ROOF',\n",
    "    'PLAYING_POLO',\n",
    "    'REMOVING_CURLERS',\n",
    "    'ELLIPTICAL_TRAINER',\n",
    "    'HEALTH',\n",
    "    'SPREAD_MULCH',\n",
    "    'CHOPPING_WOOD',\n",
    "    'BRUSHING_TEETH',\n",
    "    'USING_THE_POMMEL_HORSE',\n",
    "    'SNATCH',\n",
    "    'CLIPPING_CAT_CLAWS',\n",
    "    'PUTTING_ON_MAKEUP',\n",
    "    'HAND_WASHING_CLOTHES',\n",
    "    'HITTING_A_PINATA',\n",
    "    'TAI_CHI',\n",
    "    'GETTING_A_TATTOO',\n",
    "    'DRINKING_BEER',\n",
    "    'SHAVING_LEGS',\n",
    "    'DOING_KARATE',\n",
    "    'PLAYING_RUBIK_CUBE',\n",
    "    'FAMILY_LIFE',\n",
    "    'ROLLERBLADING',\n",
    "    'EDUCATION_AND_COMMUNICATIONS',\n",
    "    'FIXING_BICYCLE',\n",
    "    'BEER_PONG',\n",
    "    'IRONING_CLOTHES',\n",
    "    'CUTTING_THE_GRASS',\n",
    "    'RAKING_LEAVES',\n",
    "    'PLAYING_SQUASH',\n",
    "    'HOPSCOTCH',\n",
    "    'INSTALLING_CARPET',\n",
    "    'POLISHING_FURNITURE',\n",
    "    'DECORATING_THE_CHRISTMAS_TREE',\n",
    "    'PREPARING_SALAD',\n",
    "    'PREPARING_PASTA',\n",
    "    'VACUUMING_FLOOR',\n",
    "    'CLEAN_AND_JERK',\n",
    "    'COMPUTERS_AND_ELECTRONICS',\n",
    "    'CROQUET'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54218.35 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50195.51 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55186.19 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55788.79 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 57056.78 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55481.98 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53989.28 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56734.91 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 57041.33 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54562.72 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53976.97 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55568.87 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54080.30 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56100.44 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54268.58 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50963.39 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 51234.42 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53916.92 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55482.56 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54282.57 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55480.30 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 37894.36 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55461.16 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54887.02 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52031.20 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53416.19 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55775.05 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52852.68 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54581.53 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54681.09 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56736.05 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56508.08 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54000.08 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55590.94 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54875.43 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54037.01 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54801.61 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55107.27 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56114.49 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56114.34 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54936.85 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55775.94 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52078.48 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55327.26 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54901.11 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53688.27 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55481.32 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54575.66 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55775.35 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54047.27 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 47828.70 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 47379.51 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54266.48 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55480.74 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50608.29 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56087.37 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54564.98 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53420.73 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53986.86 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 51721.89 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 38187.32 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55162.84 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54576.22 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54872.29 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52576.45 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53414.29 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55466.78 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52575.85 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55493.09 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52864.36 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55688.04 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55176.06 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55685.97 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53122.86 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54874.43 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55189.22 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55483.30 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56226.03 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54901.97 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54561.66 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55791.23 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56027.46 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56415.81 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55188.93 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55176.21 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 51969.95 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55475.62 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53608.19 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55085.86 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54293.13 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53977.11 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52853.48 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53712.78 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55795.74 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54604.38 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54576.65 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56100.29 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53853.57 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54874.22 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54275.15 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55493.90 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54470.42 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54265.50 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54575.30 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55175.92 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54576.37 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 51967.70 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55480.52 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 38038.82 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53148.33 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55176.06 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52031.32 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54591.08 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 51749.53 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53996.34 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50208.73 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53245.69 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52196.46 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54292.43 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53427.98 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55161.90 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55169.63 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55486.58 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55467.44 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54955.27 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54280.82 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55802.17 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56085.72 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54280.47 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54281.52 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54575.94 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55163.27 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54575.80 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 49476.97 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54572.33 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54885.66 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53996.48 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54280.47 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55481.83 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55178.67 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55480.74 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55467.00 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54874.00 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52032.55 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55454.44 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56100.29 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53415.51 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56100.66 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50945.63 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54267.46 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52289.84 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 51107.47 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 49467.32 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56416.87 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50974.43 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50477.22 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55163.42 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55776.09 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 36258.36 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54560.74 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54281.52 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53988.87 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 51219.34 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 53401.97 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52024.13 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 55082.33 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52760.06 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54280.54 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 41270.96 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54907.41 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54285.44 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52812.19 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54576.86 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54876.44 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54875.43 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 52853.48 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50285.10 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54285.16 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54860.85 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54575.94 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 56101.26 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54576.15 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54281.31 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54575.73 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 54275.08 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 51292.32 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 50198.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_list = []\n",
    "for idx, b in enumerate(benchmarks):\n",
    "    data_list += HellaSwag().load_benchmark_dataset(HellaSwagTask[b])\n",
    "\n",
    "data = [{'input': obj.input, 'expected_output': obj.expected_output} for obj in data_list]\n",
    "pd.DataFrame(data).to_csv('HellaSwagData.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. is using tools as he works."
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\whdgh\\Desktop\\projects\\Langchain_Embedding\\.venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'metric_score': 1.0,\n",
       "  'metric_reason': 'The actual output directly indicates the correct expected output answer A, confirming he is using tools as he works.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' indicates the given 'expected output'\",\n",
    "        \"It does not matter whether the 'actual output' has full context or not.\",\n",
    "        \"If indicating to correct 'expected output' answer must be OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ") \n",
    "\n",
    "benchmark = HumanEval(\n",
    "    tasks=[HumanEvalTask.HAS_CLOSE_ELEMENTS, HumanEvalTask.SORT_NUMBERS],\n",
    "    n=100\n",
    ")\n",
    "\n",
    "metric = HallucinationMetric(threshold=0.5)\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a wise and precise chatbot.\n",
    "    With the below given questions, you will have to solve the question and answer the question instructed with several options to select\n",
    "    {question}\n",
    "    Provide only the letter corresponding to your answer (e.g., \"A\", \"B\", \"C\", etc.).\n",
    "\"\"\"\n",
    "\n",
    "llm_chain = (                     \n",
    "    PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "test_result = []\n",
    "for evaluate in data:\n",
    "    # actual_output = llm_chain.invoke({\"question\": \"It depends, some might consider the cat, while others might argue the dog.\"})\n",
    "    actual_output = llm_chain.invoke({\"question\": evaluate['input']})\n",
    "    # actual_output = \"A\"\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=evaluate['input'],\n",
    "        actual_output=actual_output,\n",
    "        expected_output=evaluate['expected_output']\n",
    "    )\n",
    "\n",
    "    metric.measure(test_case)\n",
    "    result = {'metric_score' : metric.score, 'metric_reason': metric.reason}\n",
    "    test_result.append(result)\n",
    "    break\n",
    "\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델: ChatOpenAI\n",
      "GEval 평가 평균: 1.0\n"
     ]
    }
   ],
   "source": [
    "len(test_result), test_result\n",
    "avg = statistics.mean([eval['metric_score'] for eval in test_result])\n",
    "print(f\"모델: ChatOpenAI\\nGEval 평가 평균: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_score</th>\n",
       "      <th>metric_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The actual output directly indicates the corre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   metric_score                                      metric_reason\n",
       "0           1.0  The actual output directly indicates the corre..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(test_result).to_csv('result.csv', encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
