{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import FaithfulnessMetric, ContextualPrecisionMetric, AnswerRelevancyMetric, ContextualRelevancyMetric, ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index = pc.Index(os.environ[\"PINECONE_INDEX\"])\n",
    "print(index.describe_index_stats())\n",
    "\n",
    "vectorstore = PineconeVectorStore(  \n",
    "    index=index, \n",
    "    embedding=OpenAIEmbeddings(), \n",
    "    text_key=\"text\"  \n",
    ")\n",
    "\n",
    "# base model\n",
    "gpt3_5 = ChatOpenAI(      \n",
    "    model_name='gpt-3.5-turbo',  \n",
    "    temperature=0.1  \n",
    ")\n",
    "\n",
    "T3Q = ChatOllama(\n",
    "    model=\"T3Q-ko-solar-dpo-v7.0:latest\", \n",
    "    name=\"T3Q-ko-solar-dpo-v7.0\", \n",
    "    temperature=0.1, \n",
    "    repeat_penalty=1.2,\n",
    "    top_k=10,\n",
    "    top_p=0.5\n",
    ")\n",
    "\n",
    "eeve = ChatOllama(\n",
    "    model=\"yanoljaEEVE-Korean-Instruct-10.8B-v1.0:latest\", \n",
    "    name=\"EEVE-Korean-Instruct-10.8B-v1.0\", \n",
    "    temperature=0.1,        \n",
    "    repeat_penalty=1.2,\n",
    "    top_k=10,\n",
    "    top_p=0.5\n",
    ")\n",
    "\n",
    "platypus = ChatOllama(\n",
    "    model=\"KoR-Orca-Platypus-13B:latest\", \n",
    "    name=\"KoR-Orca-Platypus-13B\", \n",
    "    temperature=0.1,        \n",
    "    repeat_penalty=1.2,\n",
    "    top_k=10,\n",
    "    top_p=0.5\n",
    ")\n",
    "\n",
    "llama3_bllossom = ChatOllama(\n",
    "    model=\"llama-3-Korean-Bllossom-8B:latest\", \n",
    "    name=\"llama-3-Korean-Bllossom-8B\", \n",
    "    temperature=0.1,        \n",
    "    repeat_penalty=1.2,\n",
    "    top_k=10,\n",
    "    top_p=0.5\n",
    ")\n",
    "\n",
    "llama3 = ChatOllama(\n",
    "    model=\"Meta-Llama-3-8B:latest\", \n",
    "    name=\"Meta-Llama-3-8B\", \n",
    "    temperature=0.1,        \n",
    "    repeat_penalty=1.2,\n",
    "    top_k=10,\n",
    "    top_p=0.5\n",
    ")\n",
    "\n",
    "def evaluate_faithfulness(llm_name, query, answer, **kwargs):\n",
    "    context = kwargs.get('context')\n",
    "    FAITHFUL = FaithfulnessMetric(\n",
    "        threshold=0.7,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=query,\n",
    "        actual_output=answer,\n",
    "        retrieval_context=context\n",
    "    )\n",
    "\n",
    "    score = -1\n",
    "    reason = \"error\"\n",
    "    try:\n",
    "        FAITHFUL.measure(test_case)\n",
    "        score = FAITHFUL.score\n",
    "        reason = FAITHFUL.reason\n",
    "        print('metric.score', score)\n",
    "        print(\"-\" * 50)\n",
    "        print('metric.reason', reason)\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print('e', e)\n",
    "        pass\n",
    "\n",
    "    test_result = {\n",
    "        'llm_name': llm_name, \n",
    "        'input': query, \n",
    "        'actual_output': answer, \n",
    "        'context': '\\n\\n'.join(context), \n",
    "        'metric_score': score, \n",
    "        'metric_reason': reason\n",
    "    }\n",
    "    return test_result\n",
    "\n",
    "def evaluate_contextual_precision(llm_name, query, answer, **kwargs):\n",
    "    expected_output = kwargs.get('expected_output')\n",
    "    context = kwargs.get('context')\n",
    "    CONTEXTUAL_PRECISION = ContextualPrecisionMetric(\n",
    "        threshold=0.7,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=query,\n",
    "        actual_output=answer,\n",
    "        expected_output=expected_output,\n",
    "        retrieval_context=context\n",
    "    )\n",
    "\n",
    "    score = -1\n",
    "    reason = \"error\"\n",
    "    try:\n",
    "        CONTEXTUAL_PRECISION.measure(test_case)\n",
    "        score = CONTEXTUAL_PRECISION.score\n",
    "        reason = CONTEXTUAL_PRECISION.reason\n",
    "        print('metric.score', score)\n",
    "        print(\"-\" * 50)\n",
    "        print('metric.reason', reason)\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print('e', e)\n",
    "        pass\n",
    "\n",
    "    test_result = {\n",
    "        'llm_name': llm_name, \n",
    "        'input': query, \n",
    "        'actual_output': answer, \n",
    "        'context': '\\n\\n'.join(context), \n",
    "        'metric_score': score, \n",
    "        'metric_reason': reason\n",
    "    }\n",
    "    return test_result\n",
    "\n",
    "def evaluate_contextual_relacancy(llm_name, query, answer, **kwargs):\n",
    "    context = kwargs.get('context')\n",
    "    CONTEXTUAL_RELAVANCY = ContextualRelevancyMetric(\n",
    "        threshold=0.7,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=query,\n",
    "        actual_output=answer,\n",
    "        retrieval_context=context\n",
    "    )\n",
    "\n",
    "    score = -1\n",
    "    reason = \"error\"\n",
    "    try:\n",
    "        CONTEXTUAL_RELAVANCY.measure(test_case)\n",
    "        score = CONTEXTUAL_RELAVANCY.score\n",
    "        reason = CONTEXTUAL_RELAVANCY.reason\n",
    "        print('metric.score', score)\n",
    "        print(\"-\" * 50)\n",
    "        print('metric.reason', reason)\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print('e', e)\n",
    "        pass\n",
    "\n",
    "    test_result = {\n",
    "        'llm_name': llm_name, \n",
    "        'input': query, \n",
    "        'actual_output': answer, \n",
    "        'context': '\\n\\n'.join(context), \n",
    "        'metric_score': score, \n",
    "        'metric_reason': reason\n",
    "    }\n",
    "    return test_result\n",
    "\n",
    "def evaluate_contextual_recall(llm_name, query, answer, expected_output, **kwargs):\n",
    "    context = kwargs.get('context')\n",
    "    CONTEXTUAL_RECALL = ContextualRecallMetric(\n",
    "        threshold=0.7,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=query,\n",
    "        actual_output=answer,\n",
    "        expected_output=expected_output,\n",
    "        retrieval_context=context\n",
    "    )\n",
    "    score = -1\n",
    "    reason = \"error\"\n",
    "    try:\n",
    "        CONTEXTUAL_RECALL.measure(test_case)\n",
    "        score = CONTEXTUAL_RECALL.score\n",
    "        reason = CONTEXTUAL_RECALL.reason\n",
    "        print('metric.score', score)\n",
    "        print(\"-\" * 50)\n",
    "        print('metric.reason', reason)\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print('e', e)\n",
    "        pass\n",
    "\n",
    "    test_result = {\n",
    "        'llm_name': llm_name, \n",
    "        'input': query, \n",
    "        'actual_output': answer, \n",
    "        'context': '\\n\\n'.join(context), \n",
    "        'metric_score': score, \n",
    "        'metric_reason': reason\n",
    "    }\n",
    "    return test_result\n",
    "\n",
    "def evaluate_answer_relavancy(llm_name, query, answer, **kwargs):\n",
    "    context = kwargs.get('context')\n",
    "    ANSWER_RELAVANCY = AnswerRelevancyMetric(\n",
    "        threshold=0.7,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=query,\n",
    "        actual_output=answer\n",
    "    )\n",
    "    score = -1\n",
    "    reason = \"error\"\n",
    "    try:\n",
    "        ANSWER_RELAVANCY.measure(test_case)\n",
    "        score = ANSWER_RELAVANCY.score\n",
    "        reason = ANSWER_RELAVANCY.reason\n",
    "        print('metric.score', score)\n",
    "        print(\"-\" * 50)\n",
    "        print('metric.reason', reason)\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print('e', e)\n",
    "        pass\n",
    "\n",
    "    test_result = {\n",
    "        'llm_name': llm_name, \n",
    "        'input': query, \n",
    "        'actual_output': answer, \n",
    "        'context': '\\n\\n'.join(context), \n",
    "        'metric_score': score, \n",
    "        'metric_reason': reason\n",
    "    }\n",
    "    return test_result\n",
    "\n",
    "llms = [\n",
    "    # gpt3_5,\n",
    "    T3Q,\n",
    "    eeve,\n",
    "    platypus,\n",
    "    llama3_bllossom,\n",
    "    llama3\n",
    "]\n",
    "\n",
    "metrics = [\n",
    "    # {'label': 'FAITHFUL', 'value': evaluate_faithfulness},\n",
    "    {'label': 'CONTEXTUAL_PRECISION', 'value': evaluate_contextual_precision},\n",
    "    {'label': 'CONTEXTUAL_RELAVANCY', 'value': evaluate_contextual_relacancy},\n",
    "    {'label': 'CONTEXTUAL_RECALL', 'value': evaluate_contextual_recall},\n",
    "    {'label': 'ANSWER_RELAVANCY', 'value': evaluate_answer_relavancy}\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a friendly and polite machine learning expert chatbot.\n",
    "Answer the user's question based on the [context] below.\n",
    "Don't answer anything that isn't in the context and say, \"I'm not sure how to answer that question, please be more specific.\"\n",
    "Don't answer in one sentence, but in 3-5 sentences.\n",
    "Give reasons and evidence.\n",
    "All answers should be in Korean.\\n\\n[context]\\n{context}\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "queries = [\n",
    "    \"온라인 학습 시스템이 무엇인가요?\",\n",
    "    \"검증 오차가 상승하면 미니배치 경사 하강법을 즉시 중단하는 것이 좋은 방법인가요?\",\n",
    "    \"릿지 회귀를 사용했을 때 훈련 오차와 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $a$를 증가시켜야 할까요, 아니면 줄여야 할까요?\",\n",
    "    \"사진을 낮과 밤, 실내와 실외로 분류하려 합니다. 두 개의 로지스틱 회귀 분류기를 만들어야 할까요, 아니면 하나의 소프트맥스 회귀 분류기를 만들어야 할까요?\",\n",
    "    \"수백만 개의 샘플과 수백 개의 특성을 가진 훈련 세트에 SVM 모델을 훈련시키려면 원 문제와 쌍대 문제 중 어떤 것을 사용해야 하나요?\",\n",
    "    \"결정 트리가 훈련 세트에 과소적합되었다면 입력 특성의 스케일을 조정하는 것이 좋을까요?\",\n",
    "    \"oob 평가의 장점은 무엇인가요?\",\n",
    "    \"차원의 저주란 무엇인가요?\",\n",
    "    \"적층 오토인코더의 하위층에서 학습한 특성을 시각화하기 위해 사용하는 일반적인 기법은 무엇인가요? 상위층에 대해서는 어떻게 할 수 있나요?\",\n",
    "    \"강화 학습 에이전트의 성능은 어떻게 측정할 수 있나요?\"\n",
    "]\n",
    "\n",
    "answers = [\n",
    "    \"온라인 학습 시스템은 배치 학습 시스템과 달리 점진적으로 학습할 수 있습니다. 이 방식은 변화하는 데이터와 자율 시스템에 빠르게 적응하고 매우 많은 양의 데이터를 훈련시킬 수 있습니다\",\n",
    "    \"무작위성 때문에 확률적 경사 하강법이나 미니배치 경사 하강법 모두 매 훈련 반복마다 학습의 진전을 보장하지 못합니다. 검증 에러가 상승될 때 훈련을 즉시 멈춘다면 최적점에 도달하기 전에 너무 일찍 멈추게 될지 모릅니다. 더 나은 방법은 정기적으로 모델을 저장하고 오랫동안 진전이 없을 때 (즉, 최상의 점수를 넘어서지 못하면), 저장된 것 중 가장 좋은 모델로 복원하는 것입니다.\",\n",
    "    \"훈련 에러와 검증 에러가 거의 비슷하고 매우 높다면 모델이 훈련 세트에 과소적합되었을 가능성이 높습니다. 즉, 높은 편향을 가진 모델입니다. 따라서 규제 하이퍼파라미터 $a$를 감소시켜야 합니다.\",\n",
    "    \"실외와 실내, 낯과 밤에 따라 사진을 구분하고 싶다면 이 둘은 배타적인 클래스가 아니기 때문에(즉, 네 가지 조합이 모두 가능하므로) 두 개의 로지스틱 회귀 분류기를 훈련시켜야 합니다.\",\n",
    "    \"커널 SVM은 쌍대 형식만 사용할 수 있기 때문에 이 질문은 선형 SVM에만 해당합니다. 원 문제의 계산 복잡도는 훈련 샘플 수에 비례하지만, 쌍대 형식의 계산 복잡도는 삼각대수에 비례합니다. 그러므로 수백만 개의 샘플이 있다면 쌍대 형식은 너무 느려질 것이므로 원 문제를 사용해야 합니다.\",\n",
    "    \"결정 트리가 훈련 세트에 과대적합되었다면 모델에 제약을 가해 규제해야 하므로 max. depth를 낮추는 것이 좋습니다.\",\n",
    "    \"oob 평가를 사용하면 배깅 앙상블의 각 예측기가 훈련에 포함되지 않은 (즉, 따로 떼어놓은) 샘플을 사용해 평가됩니다. 이는 추가적인 검증 세트가 없어도 편향되지 않게 앙상블을 평가하도록 도와줍니다. 그러므로 훈련에 더 많은 샘플을 사용할 수 있어서 앙상블의 성능은 조금 더 향상될 것입니다.\",\n",
    "    \"우리는 3차원 세계에서 살고 있어서 고차원 공간을 직관적으로 상상하기 어렵습니다. 1,000차원의 공간에서 휘어져 있는 200차원의 타원체는 고사하고 기본적인 4차원 초입방체(hypercube)조차도 머릿속에 그리기 어렵습니다. 차원의 저주는 저차원 공간에는 없는 많은 문제가 고차원 공간에서 일어난다는 사실을 뜻합니다. 머신러닝에서 무작위로 선택한 고차원 벡터는 매우 희소해서 과대적합의 위험이 크고, 많은 양의 데이터가 있지 않으면 데이터에 있는 패턴을 잡아내기 매우 어려운 것이 흔한 현상입니다.\",\n",
    "    \"스택 오토인코더의 하위층이 학습한 특성을 시각화하기 위한 일반적인 방법은 각 뉴런의 가중치를 입력 이미지의 크기로 바꾸어 그려보는 것입니다（예를 들어 MNIST에서는 [784] 크기의 가중치 벡터를 [28, 28]로 바꿈니다）. 상위층에서 학습한 특성을 시각화하기 위한 한 가지 방법은 각 뉴런을 가장 활성화시키는 훈련 샘플을 그려보는 것입니다.\",\n",
    "    \"강화 학습 에이전트의 성능을 측정하려면 간단하게 얻은 보상을 모두 더하면 됩니다. 시물레이션 환경에서는 많은 에피소드를 수행하고 평균적으로 얻은 전체 보상을 확인합니다（최소，최대，표준 편차 등을 볼 수 있습니다）\"\n",
    "]\n",
    "\n",
    "# 메모리 리스트(추가 예정)\n",
    "chat_history = []\n",
    "\n",
    "def chat_with_llm(llm, vectorstore, prompt, query):\n",
    "    retriever_chain = create_history_aware_retriever(llm, vectorstore.as_retriever(search_kwargs={\"k\": 6}), prompt)\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "    response = retrieval_chain.invoke({\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": query\n",
    "    })\n",
    "\n",
    "    return response\n",
    "\n",
    "def print_response(response, time):\n",
    "    input = response[\"input\"]\n",
    "    context = response[\"context\"]\n",
    "    context = '\\n\\n'.join([f\"[{idx}] {doc.page_content}\" for idx, doc in enumerate(context, 1)])\n",
    "    answer = response[\"answer\"]\n",
    "\n",
    "    print(f\"context: {context}\")\n",
    "    print(f\"question: {input}\")\n",
    "    print(f\"answer: {answer}({time:.2f} sec)\")\n",
    "    return input, context, answer, time\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(columns=[\"model\", \"question\", \"answer\", \"context\", \"time\"])\n",
    "# df.loc[len(df)] = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "# df.to_csv(\"result.csv\", sep=\"@\")\n",
    "\n",
    "for items in metrics:\n",
    "    label = items.get('label')\n",
    "    metric_func = items.get('value')\n",
    "    # 검증 metrics\n",
    "    for llm in llms:\n",
    "        results = []\n",
    "        llm_name = llm.name\n",
    "        \n",
    "        if llm_name is None:\n",
    "            llm_name = \"GPT-3.5\"\n",
    "\n",
    "        print('llm_name', llm_name)\n",
    "        print(\"=\" * 50)\n",
    "        for idx, query in enumerate(queries):\n",
    "            start_time = time.time()\n",
    "            print('strated', idx)\n",
    "\n",
    "            response = chat_with_llm(\n",
    "                llm,\n",
    "                vectorstore=vectorstore,\n",
    "                prompt=prompt,\n",
    "                query=query\n",
    "            )\n",
    "            input = response[\"input\"]\n",
    "            # context = '\\n\\n'.join([f\"[{idx}] {doc.page_content}\" for idx, doc in enumerate(response[\"context\"], 1)])\n",
    "            context = [f\"[{idx}] {doc.page_content}\" for idx, doc in enumerate(response[\"context\"], 1)]\n",
    "            answer = response[\"answer\"]\n",
    "\n",
    "            execution_time = time.time() - start_time\n",
    "            print('input', input)\n",
    "            # input, context, answer, _time = print_response(response, execution_time)\n",
    "            print(\"-\" * 50)\n",
    "            print('context', context)\n",
    "            print(\"-\" * 50)\n",
    "            print('answer', answer)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            test_result = metric_func(llm_name=llm_name, query=query, answer=answer, expected_output=answers[idx], context=context)\n",
    "            results.append(test_result)\n",
    "            print('inserted')\n",
    "            # df.loc[len(df)] = [llm_name, input, answer, context, _time]\n",
    "        # csv 저장\n",
    "        pd.DataFrame(results).to_csv(f'{llm_name}_{label}_result.csv', encoding='utf-8-sig')\n",
    "        print()\n",
    "\n",
    "# df.to_csv(\"result.csv\", sep=\"@\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
